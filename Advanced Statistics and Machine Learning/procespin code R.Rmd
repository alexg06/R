######################################
##        LOAD LIBRARY              ##
######################################
```{r}
#library(dtree)
library(caret)
library(stats)
library(e1071)
library(rpart)
library(mice)
library(tidyverse)
library(corrplot)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(FactoMineR)
library(xgboost)
library(neuralnet)
library(MASS)
library(glmnet)
library(randomForest)
```





######################################
##        LOAD DATA FILE            ##
######################################
```{r}
df <- read.table("procespin.txt", header=T)
head(df)
```

######################################
##        SUMMARY                   ##
######################################
```{r}
lny <- data.frame(log(df$y))
df <- cbind(lny, df)
names(df)[1] <- "lny"
df$y <- NULL
head(df)
summary(df)

```

######################################
##        MISSING VALUES?           ##
######################################
```{r}
library(mice)
md.pattern(df, plot = F)
```

######################################
##        QQPLOT & DISTRIBUTION     ##
######################################
```{r}
bins <- round(sqrt(33)) + 1
for (i in colnames(df)) {
  hist(df[,i], main = i, breaks = seq(min(df[,i]),max(df[,i]),l = bins), col="orange", freq=F) #sqrt of observations]
  lines(density(df[,i], adjust=1), col="darkblue")
  charT <- as.character(i)
  print(charT)
  txt <-  cbind(charT, " normal Q-Q plot")
  print(txt)
  qqnorm(df[,i], main = txt)
}
```


######################################
##        VAR CORRELATIONS          ##
######################################
```{r}
my.cor <- cor(df)
corrplot.mixed(my.cor, lower.col = "black", number.cex = 0.7)
c <- c()
for (i in colnames(df)) {
  test <- class(df[[i]])
  ifelse(test == "numeric" || test == "integer"  , c <- c(c, i), c)
}
```


#################################################
##       PCA VARIABLE & SAMPLES VISUALIZATION  ##
#################################################
```{r}
a <- PCA(df, scale.unit = T, ncp = 2, graph = TRUE)
```

####################################################
##        LINEAR MODEL WITH ALL VARIABLES         ##
####################################################
```{r}
model_LM <- lm(lny ~ ., data = df)
model_LM
plot(model_LM)
predict_LM <- predict(model_LM, newdata = df)
rmse_LM <- RMSE(df$lny, predict_LM)
rmse_LM
plot(predict_LM ~ df$lny)
legend <- sprintf("RMSE_LM %s", round(rmse_LM,3))
legend("topleft", legend = legend)
```
####################################################
##        VARIABLES SELECTION OF LINEAR MODEL     ##
####################################################



```{r}
model_FULL <- lm(lny ~ ., data = df)
AIC <- step(model_FULL)
AIC
```


####################################################
##        CROSSVALIDATION FOR LINEAR MODEL        ##
####################################################


```{r}
library(cvTools)
print("LM_0")
model_LM_0 <- lm(lny ~ ., data = df)
repCV(model_LM_0, K = 5)
print("LM_1")
model_LM_1 <- lm(lny ~ x9 + x4 + x5 + x2 + x1, data = df)
repCV(model_LM_1, K = 5)
print("LM_2")
model_LM_2 <- lm(lny ~ x9 + x4 + x5 + x2 + x1 + x3, data = df)
repCV(model_LM_2, K = 5)
print("LM_3")
model_LM_3 <- lm(lny ~ x9 + x4 + x5 + x2 + x1 + x3 + x10, data = df)
repCV(model_LM_3, K = 5)
print("LM_4")
model_LM_4 <- lm(lny ~ x3 + x1 + x8 + x4, data = df)
repCV(model_LM_4, K = 5)
print("LM_5")
model_LM_5 <- lm(lny ~ x1 + x2 + x4 + x5, data = df)
repCV(model_LM_5, K = 5)

anova(model_LM_0, model_LM_1)
anova(model_LM_0, model_LM_2)
anova(model_LM_0, model_LM_3)
anova(model_LM_0, model_LM_4)
anova(model_LM_0, model_LM_5)
anova(model_LM_1, model_LM_5)
anova(model_LM_4, model_LM_5)
fits <- list(model_LM_0 = model_LM_0, model_LM_1 = model_LM_1,model_LM_2 = model_LM_2,model_LM_3 = model_LM_3, model_LM_4 = model_LM_4, model_LM_5 = model_LM_5)
print(sapply(fits, AIC))
plot(model_LM_5)
```

####################################################
##                  RESIDUALS                     ##
####################################################

```{r}
library(olsrr)
plot(model_LM_5$residuals)
mean(model_LM_5$residuals)
qqnorm(model_LM_5$residuals)
ols_test_normality(model_LM_5)
ols_plot_resid_hist(model_LM_5)
```



```{r}

rmse_LM <- c()
rmse_LMBEST <- c()
rmse_FULL <- c()

for (i in 1:100) {
  set.seed(i*10+1)
  train <- sample(nrow(df), 0.7*nrow(df), replace = FALSE)
  train_Set <- df[train,]
  test_Set <- df[-train,]
  
  #####################################################################################
  model_LMBEST <- lm(lny ~ x4 + x5 + x2 + x1, data = train_Set)
  predict_LMBEST <- predict(model_LMBEST, newdata = test_Set[,-1])
  rmse_tmp <- RMSE(pred = predict_LMBEST, obs = test_Set$lny)
  rmse_LMBEST <- cbind(rmse_LMBEST, rmse_tmp)
  #####################################################################################
  model_FULL <- lm(lny ~ ., data = train_Set)
  predict_FULL <- predict(model_FULL, newdata = test_Set[,-1])
  rmse_tmp <- RMSE(pred = predict_FULL, obs = test_Set$lny)
  rmse_FULL <- cbind(rmse_FULL, rmse_tmp)

}
```


```{r}

mean(rmse_LMBEST)
hist(rmse_LMBEST, breaks = 20)
mean(rmse_FULL)
hist(rmse_FULL, breaks = 20)
```

```{r}
LM_best <- c(mean(rmse_LMBEST), sd(rmse_LMBEST), min(rmse_LMBEST), max(rmse_LMBEST))
LM_full <- c(mean(rmse_FULL), sd(rmse_FULL), min(rmse_FULL), max(rmse_FULL))
a <- data.frame(LM_full)
a <-cbind(a, LM_best)
rownames(a) <- c("RMSE", "SD", "MIN", "MAX")
a
```
```{r}

print("###########################LM_BEST #####################")
print("#################################################")
summary(model_LMBEST)
print("#################################################")
print("#################### LM_FULL  ###################")
print("#################################################")
summary(model_FULL)
```


####################################################
##        VARIABLES SELECTION with LASSO          ##
####################################################

```{r}
lasso <- cv.glmnet(as.matrix(df[,2:11]), as.matrix(df$lny), alpha = 1)
coef(lasso)
plot(lasso)
```


```{r}
lasso$lambda.min
best_model <- which(lasso$lambda==lasso$lambda.min)
lasso$glmnet.fit$beta[,best_model]
```



####################################################
##        FULL DECISION TREE BUILDING w/ CP = 0   ##
####################################################
```{r}
mtree <- rpart(lny ~ ., data = df, cp = 0, minbucket = 1, minsplit=1, method = 'anova')
mtree
plot(mtree)
text(mtree)
fancyRpartPlot(mtree, uniform=TRUE, main="Max regression Tree",)
plotcp(mtree)
```


####################################################
##        BAGGING MOST USEFUL VARIABLES           ##
####################################################
```{r}
bagged_dt <- train(lny ~ ., data = df,  method = "treebag",  importance = TRUE  )

# assess results
bagged_dt

# plot most important variables
plot(varImp(bagged_dt), 20)
```
#####################################################
##        FULL DECISION TREE WITH 4 BEST VARIABLES ##
#####################################################
```{r}
mtree <- rpart(lny ~ x1 + x8 + x3 + x6 + x9 + x4 + x2 + x5, data = df, cp = 0, minbucket = 1, minsplit=1, method = 'anova', control = list(cp = 0, xval = 20))
mtree
plot(mtree)
text(mtree)
fancyRpartPlot(mtree, uniform=TRUE, main="Max regression Tree",)
plotcp(mtree)
```

#####################################################
##        PRUNED DECISION TREE                     ##
#####################################################
```{r}
ptree<- prune(mtree, cp= 0.02)
fancyRpartPlot(ptree, uniform=TRUE, main="Pruned Tree")
plotcp(ptree)
```



#####################################################
##        RANDOM FOREST PART                       ##
#####################################################
```{r}
model_RF <- randomForest(lny ~ x1 + x8 + x3 + x6 + x9 + x4 + x2 + x5, data = df)
summary(model_RF)
plot(model_RF)
```
```{r}
bagged_rf <- train(lny ~ ., data = df,  method = "rf",  importance = TRUE  )

# assess results
bagged_rf

# plot most important variables
plot(varImp(bagged_rf), 20)
```


################################
##        SVM PART            ##
################################
```{r}
model_SVM <- svm(lny ~ ., data = df)
summary(model_SVM)
```


################################
##        XGBOOST PART        ##
################################
```{r}
model_XGB <- xgboost(data = as.matrix(df[,2:11]), label = df$lny, max_depth = 9, eta = 1, nthread = 2, nrounds = 3, verbose = 1)
summary(model_XGB)
```


```{r}
rmse_DT <- c()
rmse_RF <- c()
rmse_LM2 <- c()

for (i in 1:20) {
  set.seed(i*10+1)
  train <- sample(nrow(df), 0.7*nrow(df), replace = FALSE)
  train_Set <- df[train,]
  test_Set <- df[-train,]
  #####################################################################################
  model_DT <- rpart(lny ~ x1 + x8 + x3 + x6 + x9 + x4 + x2 + x10, data = train_Set, cp = 0, minbucket = 1, minsplit=1, method = 'anova', control = list(cp = 0.02, xval = 20))
  prune_DT <- prune(model_DT, cp= 0.02)
  predict_DT <- predict(prune_DT, newdata = test_Set[,-1], type = 'vector')
  rmse_tmp <- RMSE(pred = predict_DT, obs = test_Set$lny)
  rmse_DT <- cbind(rmse_DT, rmse_tmp)
  #####################################################################################
  model_RF <- randomForest(lny ~ x1 + x8 + x3 + x6 + x9 + x4 + x2 + x10 , data = train_Set, ntree = 500)
  predict_RF <- predict(model_RF, newdata = test_Set[,2:11])
  rmse_tmp <- RMSE(test_Set$lny, predict_RF)
  rmse_RF <- cbind(rmse_RF, rmse_tmp)
  #####################################################################################
  model_LM2 <- lm(lny ~ x4 + x5 + x2 + x1, data = train_Set)
  predict_LM2 <- predict(model_LM2, newdata = test_Set[,-1])
  rmse_tmp <- RMSE(pred = predict_LM2, obs = test_Set$lny)
  rmse_LM2 <- cbind(rmse_LM2, rmse_tmp)

}
```
```{r}
par(mfrow=c(2,2)) # init 4 charts in 1 panel

```


```{r}
mean(rmse_DT)
hist(rmse_DT, breaks = 10)
min(rmse_DT)
mean(rmse_RF)
hist(rmse_RF, breaks = 10)
mean(rmse_LM2)
hist(rmse_LM2, breaks = 10)
```

```{r}
DT <- c(mean(rmse_DT), sd(rmse_DT), min(rmse_DT), max(rmse_DT))
RF <- c(mean(rmse_RF), sd(rmse_RF), min(rmse_RF), max(rmse_RF))
LM_full <- c(mean(rmse_LM), sd(rmse_LM), min(rmse_LM), max(rmse_LM))
LM_best <- c(mean(rmse_LM2), sd(rmse_LM2), min(rmse_LM2), max(rmse_LM2))
a <- data.frame(DT)
a <-cbind(a, LM_best, RF)
rownames(a) <- c("RMSE", "SD", "MIN", "MAX")
a
```